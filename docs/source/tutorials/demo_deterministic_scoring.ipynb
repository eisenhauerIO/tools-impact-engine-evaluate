{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Deterministic Confidence Scoring\n\nThis tutorial demonstrates the deterministic evaluation path — a lightweight\nscorer included for debugging, testing, and illustration. It assigns a\nreproducible confidence score to a causal estimate based on the measurement\nmethodology, without calling an LLM.\n\n## Workflow overview\n\n1. Create a mock job directory with `manifest.json` and `impact_results.json`\n2. Score directly with `score_initiative()`\n3. Score via the `Evaluate` adapter\n4. Verify reproducibility across calls"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "from notebook_support import print_result_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a mock job directory\n",
    "\n",
    "The measure stage writes a job directory containing `manifest.json` (metadata\n",
    "and file references) and `impact_results.json` (the measurement output). Here\n",
    "we create one manually to simulate what the measure stage produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tempfile.mkdtemp(prefix=\"job-impact-engine-\")\n",
    "job_dir = Path(tmp)\n",
    "\n",
    "manifest = {\n",
    "    \"schema_version\": \"2.0\",\n",
    "    \"model_type\": \"experiment\",\n",
    "    \"evaluate_strategy\": \"deterministic\",\n",
    "    \"created_at\": \"2025-06-01T12:00:00+00:00\",\n",
    "    \"files\": {\n",
    "        \"impact_results\": {\"path\": \"impact_results.json\", \"format\": \"json\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "impact_results = {\n",
    "    \"ci_upper\": 15.0,\n",
    "    \"effect_estimate\": 10.0,\n",
    "    \"ci_lower\": 5.0,\n",
    "    \"cost_to_scale\": 100.0,\n",
    "    \"sample_size\": 500,\n",
    "}\n",
    "\n",
    "(job_dir / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "(job_dir / \"impact_results.json\").write_text(json.dumps(impact_results, indent=2))\n",
    "\n",
    "print(f\"Job directory: {job_dir}\")\n",
    "print(f\"Files: {[p.name for p in job_dir.iterdir()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Score directly with `score_initiative()`\n\n`score_initiative()` is a pure function useful for debugging and testing. It\ntakes a scorer event dict and a confidence range, hashes the `initiative_id`\nto seed an RNG, and draws a reproducible confidence value. The confidence\nrange comes from the method reviewer (an experiment uses `(0.85, 1.0)` because\nRCTs produce the strongest evidence)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from impact_engine_evaluate import score_initiative\n",
    "\n",
    "event = {\n",
    "    \"initiative_id\": \"initiative-demo-001\",\n",
    "    \"model_type\": \"experiment\",\n",
    "    \"ci_upper\": 15.0,\n",
    "    \"effect_estimate\": 10.0,\n",
    "    \"ci_lower\": 5.0,\n",
    "    \"cost_to_scale\": 100.0,\n",
    "    \"sample_size\": 500,\n",
    "}\n",
    "\n",
    "result = score_initiative(event, confidence_range=(0.85, 1.0))\n",
    "print_result_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence score falls within `(0.85, 1.0)` because we specified the\n",
    "experiment confidence range. The score is deterministic: running the same\n",
    "`initiative_id` always produces the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Score via the Evaluate adapter\n",
    "\n",
    "In the full pipeline, the orchestrator calls `Evaluate.execute()` instead of\n",
    "`score_initiative()` directly. The adapter reads the manifest, looks up the\n",
    "registered method reviewer for `model_type`, and dispatches on\n",
    "`evaluate_strategy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from impact_engine_evaluate import Evaluate\n",
    "\n",
    "evaluator = Evaluate()\n",
    "result = evaluator.execute({\"job_dir\": str(job_dir)})\n",
    "\n",
    "print_result_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adapter produces the same 8-key output dict. It automatically read\n",
    "`manifest.json`, found `evaluate_strategy: \"deterministic\"`, and used the\n",
    "experiment reviewer's confidence range `(0.85, 1.0)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify reproducibility\n",
    "\n",
    "The deterministic scorer hashes `initiative_id` to seed a random number\n",
    "generator. The same ID always produces the same confidence score, regardless\n",
    "of when or where the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [score_initiative(event, confidence_range=(0.85, 1.0))[\"confidence\"] for _ in range(5)]\n",
    "\n",
    "print(f\"Scores across 5 calls: {scores}\")\n",
    "assert len(set(scores)) == 1, \"Scores should be identical\"\n",
    "print(\"All scores are identical — deterministic scoring is reproducible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare confidence ranges across methods\n",
    "\n",
    "Different measurement methodologies get different confidence ranges. An RCT\n",
    "deserves higher confidence than a weaker design. The `MethodReviewerRegistry`\n",
    "exposes the confidence map for all registered methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from impact_engine_evaluate.review.methods import MethodReviewerRegistry\n",
    "\n",
    "print(\"Registered methods and confidence ranges:\")\n",
    "for name, bounds in sorted(MethodReviewerRegistry.confidence_map().items()):\n",
    "    print(f\"  {name}: ({bounds[0]:.2f}, {bounds[1]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThe deterministic scorer is a lightweight tool for debugging, testing, and\nillustrating the pipeline without an LLM dependency.\n\n- `score_initiative()` is a pure function that assigns deterministic\n  confidence scores.\n- Confidence ranges are tied to measurement methodology, not to individual\n  results.\n- The `Evaluate` adapter wraps this behind a job-directory-based interface\n  for the orchestrator pipeline.\n- Scoring is fully reproducible: same `initiative_id` always yields the same\n  confidence.\n\nFor production evaluation, use the **agentic** strategy, which sends\nmeasurement artifacts to an LLM for structured, per-dimension review."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Clean up\n",
    "shutil.rmtree(job_dir, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}