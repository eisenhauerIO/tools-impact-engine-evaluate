{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Agentic Review\n\nThis tutorial walks through the agentic evaluation path. It sends measurement\nartifacts to an LLM for structured, per-dimension review with scores and\njustifications.\n\n```{note}\nThis notebook requires an LLM API key and is not executed during the docs\nbuild. Code cells include pre-computed output.\n```\n\n## Workflow overview\n\n1. Prepare a job directory\n2. Configure the LLM backend\n3. Run `review()`\n4. Inspect the `ReviewResult`\n5. Examine the output files"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install the package with an LLM backend extra:\n",
    "\n",
    "```bash\n",
    "pip install \"impact-engine-evaluate[anthropic]\"\n",
    "```\n",
    "\n",
    "Set the API key:\n",
    "\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=\"sk-ant-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Prepare a job directory\n\nAn upstream producer writes a job directory with `manifest.json` and\n`impact_results.json`. Here we create one manually with realistic\nexperiment results."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job directory: /tmp/job-impact-engine-review-demo\n",
      "Files: ['manifest.json', 'impact_results.json']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "job_dir = Path(\"/tmp/job-impact-engine-review-demo\")\n",
    "job_dir.mkdir(exist_ok=True)\n",
    "\n",
    "manifest = {\n",
    "    \"model_type\": \"experiment\",\n",
    "    \"evaluate_strategy\": \"agentic\",\n",
    "    \"created_at\": \"2025-06-01T12:00:00+00:00\",\n",
    "    \"files\": {\n",
    "        \"impact_results\": {\"path\": \"impact_results.json\", \"format\": \"json\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "impact_results = {\n",
    "    \"model_type\": \"experiment\",\n",
    "    \"ci_upper\": 15.2,\n",
    "    \"effect_estimate\": 10.5,\n",
    "    \"ci_lower\": 5.8,\n",
    "    \"cost_to_scale\": 250.0,\n",
    "    \"sample_size\": 1200,\n",
    "    \"data\": {\n",
    "        \"model_params\": {\n",
    "            \"dependent_variable\": \"revenue\",\n",
    "            \"treatment_variable\": \"treatment\",\n",
    "            \"covariates\": [\"region\", \"segment\"],\n",
    "        },\n",
    "        \"impact_estimates\": {\n",
    "            \"effect_estimate\": 10.5,\n",
    "            \"ci_lower\": 5.8,\n",
    "            \"ci_upper\": 15.2,\n",
    "            \"p_value\": 0.001,\n",
    "            \"standard_error\": 2.4,\n",
    "        },\n",
    "        \"model_summary\": {\n",
    "            \"r_squared\": 0.42,\n",
    "            \"f_statistic\": 38.7,\n",
    "            \"n_observations\": 1200,\n",
    "            \"n_treatment\": 600,\n",
    "            \"n_control\": 600,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "(job_dir / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "(job_dir / \"impact_results.json\").write_text(json.dumps(impact_results, indent=2))\n",
    "\n",
    "print(f\"Job directory: {job_dir}\")\n",
    "print(f\"Files: {[p.name for p in sorted(job_dir.iterdir())]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Configure the backend\n\nThe review engine needs to know which LLM to call. The recommended approach\nis a YAML config file — reusable across many jobs and easy to swap backends:\n\n```yaml\nbackend:\n  model: \"claude-sonnet-4-6\"   # or \"ollama_chat/llama3.2\" for local\n  temperature: 0.0\n  max_tokens: 4096\n```\n\nPass the file path to `evaluate_confidence()`. A dict also works for quick\nexperiments."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"backend\": {\n",
    "        \"model\": \"claude-sonnet-4-6\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Run `evaluate_confidence()`\n\n`evaluate_confidence(config, job_dir)` is the package-level entry point.\nIt reads the manifest, dispatches to the registered reviewer, renders the\nprompt with domain knowledge, calls the LLM, parses the structured response,\nand writes `evaluate_result.json` and `review_result.json` to the job directory."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from impact_engine_evaluate import evaluate_confidence\n",
    "\n",
    "result = evaluate_confidence(config, job_dir)\n",
    "print(f\"Review complete. Overall score: {result.confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Inspect the EvaluateResult\n\nThe result contains the confidence score, strategy, and a full per-dimension\nbreakdown accessible via `result.report`:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initiative : {result.initiative_id}\")\n",
    "print(f\"Strategy   : {result.strategy}\")\n",
    "print(f\"Confidence : {result.confidence:.2f}\")\n",
    "print(f\"Range      : [{result.confidence_range[0]:.2f}, {result.confidence_range[1]:.2f}]\")\n",
    "print()\n",
    "print(\"Dimensions (from result.report):\")\n",
    "for dim in result.report[\"dimensions\"]:\n",
    "    print(f\"  {dim['name']:30s} {dim['score']:.2f}  {dim['justification'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment reviewer evaluates five dimensions:\n",
    "\n",
    "| Dimension | What it checks |\n",
    "|-----------|---------------|\n",
    "| `randomization_integrity` | Covariate balance between treatment and control |\n",
    "| `specification_adequacy` | OLS formula, covariates, functional form |\n",
    "| `statistical_inference` | CIs, p-values, F-statistic, multiple testing |\n",
    "| `threats_to_validity` | Attrition, non-compliance, spillover, SUTVA |\n",
    "| `effect_size_plausibility` | Whether the treatment effect is realistic |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Examine the output files\n\nAfter review, the evaluate stage writes `review_result.json` alongside the\noriginal artifacts. The manifest is treated as read-only — it is not modified."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = json.loads((job_dir / \"review_result.json\").read_text())\n",
    "print(\"Review result keys:\", list(review_data.keys()))\n",
    "print(f\"Overall score: {review_data['overall_score']}\")\n",
    "print(f\"Dimensions: {len(review_data['dimensions'])}\")\n",
    "print()\n",
    "print(\"Job directory contents:\")\n",
    "print([p.name for p in sorted(job_dir.iterdir())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The job directory now contains:\n\n```\njob-impact-engine-review-demo/\n├── manifest.json          # read-only (created by the producer)\n├── impact_results.json    # original upstream output\n└── review_result.json     # structured review from the LLM\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pipeline integration\n\nIn the orchestrator pipeline, the `Evaluate` component (defined in `impact_engine_orchestrator`) wraps `evaluate_confidence()` behind a unified `PipelineComponent` interface. It accepts `event[\"job_dir\"]` and returns the same `EvaluateResult` fields as a plain dict.\n\nThe `evaluate` package is a pure science library — orchestration adapters live in the orchestrator."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from impact_engine_orchestrator.components.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluator = Evaluate(config=config)\n",
    "output = evaluator.execute({\"job_dir\": str(job_dir)})\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n- `evaluate_confidence(config, job_dir)` is the package-level entry point —\n  symmetric with `evaluate_impact()` in the measure component.\n- The config file specifies the LLM backend; the job directory is a per-call\n  runtime argument pointing to existing artifacts.\n- The experiment reviewer evaluates five methodology-specific dimensions; the\n  overall confidence score is returned as `result.confidence`.\n- Per-dimension detail is accessible via `result.report[\"dimensions\"]`.\n- Results are written to `evaluate_result.json` and `review_result.json`\n  in the job directory.\n- The manifest is read-only — evaluate never modifies it.\n- For pipeline use, the orchestrator's `Evaluate` component wraps\n  `evaluate_confidence()` behind a `PipelineComponent` interface."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
