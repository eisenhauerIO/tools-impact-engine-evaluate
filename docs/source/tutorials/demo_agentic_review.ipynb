{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Agentic Review\n\nThis tutorial walks through the agentic evaluation path. It sends measurement\nartifacts to an LLM for structured, per-dimension review with scores and\njustifications.\n\n```{note}\nThis notebook requires an LLM API key and is not executed during the docs\nbuild. Code cells include pre-computed output.\n```\n\n## Workflow overview\n\n1. Prepare a job directory\n2. Configure the LLM backend\n3. Run `review()`\n4. Inspect the `ReviewResult`\n5. Examine the output files"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install the package with an LLM backend extra:\n",
    "\n",
    "```bash\n",
    "pip install \"impact-engine-evaluate[anthropic]\"\n",
    "```\n",
    "\n",
    "Set the API key:\n",
    "\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=\"sk-ant-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Prepare a job directory\n\nAn upstream producer writes a job directory with `manifest.json` and\n`impact_results.json`. Here we create one manually with realistic\nexperiment results."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job directory: /tmp/job-impact-engine-review-demo\n",
      "Files: ['manifest.json', 'impact_results.json']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "job_dir = Path(\"/tmp/job-impact-engine-review-demo\")\n",
    "job_dir.mkdir(exist_ok=True)\n",
    "\n",
    "manifest = {\n",
    "    \"schema_version\": \"2.0\",\n",
    "    \"model_type\": \"experiment\",\n",
    "    \"evaluate_strategy\": \"agentic\",\n",
    "    \"created_at\": \"2025-06-01T12:00:00+00:00\",\n",
    "    \"files\": {\n",
    "        \"impact_results\": {\"path\": \"impact_results.json\", \"format\": \"json\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "impact_results = {\n",
    "    \"schema_version\": \"2.0\",\n",
    "    \"model_type\": \"experiment\",\n",
    "    \"ci_upper\": 15.2,\n",
    "    \"effect_estimate\": 10.5,\n",
    "    \"ci_lower\": 5.8,\n",
    "    \"cost_to_scale\": 250.0,\n",
    "    \"sample_size\": 1200,\n",
    "    \"data\": {\n",
    "        \"model_params\": {\n",
    "            \"dependent_variable\": \"revenue\",\n",
    "            \"treatment_variable\": \"treatment\",\n",
    "            \"covariates\": [\"region\", \"segment\"],\n",
    "        },\n",
    "        \"impact_estimates\": {\n",
    "            \"effect_estimate\": 10.5,\n",
    "            \"ci_lower\": 5.8,\n",
    "            \"ci_upper\": 15.2,\n",
    "            \"p_value\": 0.001,\n",
    "            \"standard_error\": 2.4,\n",
    "        },\n",
    "        \"model_summary\": {\n",
    "            \"r_squared\": 0.42,\n",
    "            \"f_statistic\": 38.7,\n",
    "            \"n_observations\": 1200,\n",
    "            \"n_treatment\": 600,\n",
    "            \"n_control\": 600,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "(job_dir / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "(job_dir / \"impact_results.json\").write_text(json.dumps(impact_results, indent=2))\n",
    "\n",
    "print(f\"Job directory: {job_dir}\")\n",
    "print(f\"Files: {[p.name for p in sorted(job_dir.iterdir())]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure the backend\n",
    "\n",
    "The review engine needs to know which LLM to call. Configuration can come\n",
    "from a YAML file, a dict, or environment variables. Here we use a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"backend\": {\n",
    "        \"type\": \"anthropic\",\n",
    "        \"model\": \"claude-sonnet-4-5-20250929\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run `review()`\n",
    "\n",
    "The `review()` function reads the manifest, dispatches to the experiment\n",
    "reviewer, renders the prompt with domain knowledge, calls the LLM, parses\n",
    "the structured response, writes `review_result.json` back to the job\n",
    "directory, and returns a `ReviewResult`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review complete. Overall score: 0.82\n"
     ]
    }
   ],
   "source": [
    "from impact_engine_evaluate import review\n",
    "\n",
    "result = review(job_dir, config=config)\n",
    "print(f\"Review complete. Overall score: {result.overall_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inspect the ReviewResult\n",
    "\n",
    "The result contains per-dimension scores with justifications, an overall\n",
    "score (the mean of dimension scores), and the raw LLM response for audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiative:     job-impact-engine-review-demo\n",
      "Prompt:         experiment_review v1.0\n",
      "Backend:        anthropic (claude-sonnet-4-5-20250929)\n",
      "Overall score:  0.82\n",
      "\n",
      "Dimensions:\n",
      "  randomization_integrity:   0.90  Balanced treatment/control split (600/600)...\n",
      "  specification_adequacy:    0.85  OLS with covariates (region, segment)...\n",
      "  statistical_inference:     0.88  Strong p-value (0.001), narrow CI...\n",
      "  threats_to_validity:       0.70  No attrition data reported...\n",
      "  effect_size_plausibility:  0.78  Effect of 10.5 is within plausible range...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initiative:     {result.initiative_id}\")\n",
    "print(f\"Prompt:         {result.prompt_name} v{result.prompt_version}\")\n",
    "print(f\"Backend:        {result.backend_name} ({result.model})\")\n",
    "print(f\"Overall score:  {result.overall_score:.2f}\")\n",
    "print()\n",
    "print(\"Dimensions:\")\n",
    "for dim in result.dimensions:\n",
    "    print(f\"  {dim.name:30s} {dim.score:.2f}  {dim.justification[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment reviewer evaluates five dimensions:\n",
    "\n",
    "| Dimension | What it checks |\n",
    "|-----------|---------------|\n",
    "| `randomization_integrity` | Covariate balance between treatment and control |\n",
    "| `specification_adequacy` | OLS formula, covariates, functional form |\n",
    "| `statistical_inference` | CIs, p-values, F-statistic, multiple testing |\n",
    "| `threats_to_validity` | Attrition, non-compliance, spillover, SUTVA |\n",
    "| `effect_size_plausibility` | Whether the treatment effect is realistic |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Examine the output files\n\nAfter review, the evaluate stage writes `review_result.json` alongside the\noriginal artifacts. The manifest is treated as read-only — it is not modified."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "review_data = json.loads((job_dir / \"review_result.json\").read_text())\nprint(\"Review result keys:\", list(review_data.keys()))\nprint(f\"Overall score: {review_data['overall_score']}\")\nprint(f\"Dimensions: {len(review_data['dimensions'])}\")\nprint()\nprint(\"Job directory contents:\")\nprint([p.name for p in sorted(job_dir.iterdir())])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The job directory now contains:\n\n```\njob-impact-engine-review-demo/\n├── manifest.json          # read-only (created by the producer)\n├── impact_results.json    # original upstream output\n└── review_result.json     # structured review from the LLM\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Evaluate adapter\n",
    "\n",
    "In the orchestrator pipeline, `Evaluate.execute()` wraps `review()` behind\n",
    "a unified interface. When `evaluate_strategy` is `\"agentic\"`, the adapter\n",
    "calls `review()` internally and uses `overall_score` as the confidence\n",
    "value in the common 8-key output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"initiative_id\": \"job-impact-engine-review-demo\",\n",
      "  \"confidence\": 0.82,\n",
      "  \"cost\": 250.0,\n",
      "  \"return_best\": 15.2,\n",
      "  \"return_median\": 10.5,\n",
      "  \"return_worst\": 5.8,\n",
      "  \"model_type\": \"experiment\",\n",
      "  \"sample_size\": 1200\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from impact_engine_evaluate import Evaluate\n",
    "\n",
    "evaluator = Evaluate(config=config)\n",
    "output = evaluator.execute({\"job_dir\": str(job_dir)})\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n- `review()` runs an end-to-end LLM review of a job directory.\n- The experiment reviewer evaluates five methodology-specific dimensions.\n- Results are written to the job directory as `review_result.json`.\n- The manifest is read-only — evaluate never modifies it.\n- The `Evaluate` adapter wraps this behind a unified interface for the\n  orchestrator.\n- The `overall_score` from the review becomes the `confidence` value\n  downstream."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}