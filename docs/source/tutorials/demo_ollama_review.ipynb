{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Local LLM Review with Ollama\n",
    "\n",
    "This tutorial demonstrates the review evaluation path using a locally hosted\n",
    "model via [Ollama](https://ollama.com). No API key or internet connection is\n",
    "required — the model runs entirely on your machine.\n",
    "\n",
    "```{note}\n",
    "This notebook requires Ollama to be running locally and is not executed\n",
    "during the docs build. Code cells include pre-computed output.\n",
    "```\n",
    "\n",
    "## Workflow overview\n",
    "\n",
    "1. Inspect the job directory — a synthetic RCT with realistic artifacts\n",
    "2. Configure the backend to call `ollama_chat/llama3.2`\n",
    "3. Run `review()`\n",
    "4. Inspect the `ReviewResult`\n",
    "5. Examine the output file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install and start Ollama, then pull a model:\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "ollama serve          # already running if the desktop app is open\n",
    "```\n",
    "\n",
    "No extra Python dependencies are needed beyond the base install:\n",
    "\n",
    "```bash\n",
    "pip install impact-engine-evaluate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Step 1: Inspect the job directory\n",
    "\n",
    "The `rct_job/` directory alongside this notebook is a synthetic early-literacy\n",
    "RCT. It contains:\n",
    "\n",
    "- `manifest.json` — metadata, file references, and evaluation strategy\n",
    "- `impact_results.json` — summary statistics (effect estimate, CI, sample size)\n",
    "- `regression_output.json` — full OLS output with balance check and attrition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job directory: rct_job\n",
      "Files: ['impact_results.json', 'manifest.json', 'regression_output.json', 'review_result.json']\n",
      "\n",
      "manifest.json:\n",
      "{\n",
      "  \"schema_version\": \"2.0\",\n",
      "  \"model_type\": \"experiment\",\n",
      "  \"evaluate_strategy\": \"review\",\n",
      "  \"initiative_id\": \"literacy-rct-2024\",\n",
      "  \"created_at\": \"2025-03-15T09:00:00+00:00\",\n",
      "  \"files\": {\n",
      "    \"impact_results\": {\"path\": \"impact_results.json\", \"format\": \"json\"},\n",
      "    \"regression_output\": {\"path\": \"regression_output.json\", \"format\": \"json\"}\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "JOB_DIR = Path(\"rct_job\")\n",
    "\n",
    "print(f\"Job directory: {JOB_DIR}\")\n",
    "print(f\"Files: {sorted(p.name for p in JOB_DIR.iterdir())}\")\n",
    "print()\n",
    "print(\"manifest.json:\")\n",
    "manifest = json.loads((JOB_DIR / \"manifest.json\").read_text())\n",
    "print(json.dumps(manifest, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Configure the backend\n",
    "\n",
    "Pass a config dict to specify the model. litellm routes `ollama_chat/` prefixed\n",
    "model names to `http://localhost:11434` automatically — no extra configuration\n",
    "is needed. Swap the model name to use any model you have pulled locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"backend\": {\n",
    "        \"model\": \"ollama_chat/llama3.2\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 2048,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 3: Run `review()`\n",
    "\n",
    "The `review()` function:\n",
    "\n",
    "1. Reads `manifest.json` and loads the registered `ExperimentReviewer`\n",
    "2. Concatenates all artifact files into a single text payload\n",
    "3. Renders the prompt with domain knowledge from `knowledge/`\n",
    "4. Calls the model via litellm and parses the structured JSON response\n",
    "5. Writes `review_result.json` back to the job directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review complete. Overall score: 0.75\n"
     ]
    }
   ],
   "source": [
    "from impact_engine_evaluate.review.api import review\n",
    "\n",
    "result = review(JOB_DIR, config=config)\n",
    "print(f\"Review complete. Overall score: {result.overall_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 4: Inspect the ReviewResult\n",
    "\n",
    "The result contains per-dimension scores with justifications and an overall\n",
    "score. Five dimensions are evaluated for experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiative  : literacy-rct-2024\n",
      "Model       : ollama_chat/llama3.2\n",
      "Prompt      : experiment_review v1.0\n",
      "Timestamp   : 2026-02-28T20:44:30.894861+00:00\n",
      "\n",
      "Overall score : 0.750\n",
      "\n",
      "Dimensions:\n",
      "  randomization_integrity        0.800  |################    |\n",
      "    The study reports a balanced attrition rate (5%) and differential attrition\n",
      "    test p-value of 0.61, indicating that the treatment effect estimate is\n",
      "    likely unbiased under worst-case attrition scenarios.\n",
      "\n",
      "  specification_adequacy         0.900  |##################  |\n",
      "    The study uses a well-specified OLS model with robust standard errors\n",
      "    (HC2 heteroskedasticity-robust) and reports the F-statistic for covariate\n",
      "    balance, indicating that the model is adequately specified.\n",
      "\n",
      "  statistical_inference          0.700  |##############      |\n",
      "    The study reports a p-value of 0.007 for the treatment coefficient, which\n",
      "    is statistically significant. However, the confidence intervals (CI) are\n",
      "    not reported in a way that allows for easy interpretation of their\n",
      "    coverage probability.\n",
      "\n",
      "  threats_to_validity            0.600  |############        |\n",
      "    The study does not report any evidence of spillover effects or Hawthorne\n",
      "    effects, but it is unclear whether the sample size is sufficient to detect\n",
      "    such effects. The differential attrition test p-value of 0.61 suggests\n",
      "    that attrition may be balanced across arms, but this result should be\n",
      "    interpreted with caution.\n",
      "\n",
      "  effect_size_plausibility       0.800  |################    |\n",
      "    The estimated treatment effect (ATE) is 0.18 standardised score points,\n",
      "    which is a plausible effect size given the context of an early literacy\n",
      "    intervention in primary schools.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initiative  : {result.initiative_id}\")\n",
    "print(f\"Model       : {result.model}\")\n",
    "print(f\"Prompt      : {result.prompt_name} v{result.prompt_version}\")\n",
    "print(f\"Timestamp   : {result.timestamp}\")\n",
    "print(f\"\\nOverall score : {result.overall_score:.3f}\")\n",
    "print(\"\\nDimensions:\")\n",
    "for dim in result.dimensions:\n",
    "    bar = \"#\" * int(dim.score * 20)\n",
    "    print(f\"  {dim.name:<30} {dim.score:.3f}  |{bar:<20}|\")\n",
    "    print(f\"    {dim.justification}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "The experiment reviewer evaluates five dimensions:\n",
    "\n",
    "| Dimension | What it checks |\n",
    "|-----------|---------------|\n",
    "| `randomization_integrity` | Attrition, balance, differential dropout |\n",
    "| `specification_adequacy` | OLS formula, covariates, robust SEs |\n",
    "| `statistical_inference` | CIs, p-values, F-statistic, multiple testing |\n",
    "| `threats_to_validity` | Spillover, non-compliance, SUTVA, Hawthorne |\n",
    "| `effect_size_plausibility` | Whether the treatment effect is realistic |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 5: Examine the output file\n",
    "\n",
    "`review()` writes `review_result.json` to the job directory alongside the\n",
    "original artifacts. The manifest is treated as read-only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job directory contents: ['impact_results.json', 'manifest.json', 'regression_output.json', 'review_result.json']\n",
      "\n",
      "review_result.json keys: ['initiative_id', 'prompt_name', 'prompt_version', 'backend_name', 'model', 'dimensions', 'overall_score', 'raw_response', 'timestamp']\n",
      "Overall score : 0.75\n",
      "Dimensions    : 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job directory contents: {sorted(p.name for p in JOB_DIR.iterdir())}\")\n",
    "print()\n",
    "review_data = json.loads((JOB_DIR / \"review_result.json\").read_text())\n",
    "print(f\"review_result.json keys: {list(review_data.keys())}\")\n",
    "print(f\"Overall score : {review_data['overall_score']}\")\n",
    "print(f\"Dimensions    : {len(review_data['dimensions'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "The job directory now contains:\n",
    "\n",
    "```\n",
    "rct_job/\n",
    "├── manifest.json           # read-only (created by the producer)\n",
    "├── impact_results.json     # summary statistics\n",
    "├── regression_output.json  # full OLS output\n",
    "└── review_result.json      # structured review written by evaluate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- `review()` runs an end-to-end LLM review of a job directory — no API key\n",
    "  needed when using a local Ollama model.\n",
    "- The `ollama_chat/<model>` prefix routes requests to `http://localhost:11434`\n",
    "  via litellm; swap the model name to use any locally available model.\n",
    "- The experiment reviewer evaluates five methodology-specific dimensions and\n",
    "  returns scores with free-text justifications.\n",
    "- Results are written to `review_result.json` in the job directory.\n",
    "- The manifest is read-only — `evaluate` never modifies it.\n",
    "\n",
    "For cloud-hosted models (Anthropic, OpenAI), see the\n",
    "[Agentic Review](demo_agentic_review.ipynb) tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
